#First we load the dataset, give it reasonable column names, and split it up into a training and a testing subset.
rm(list=ls())
setwd('~/github/StatML/Assignment2')
load('./Data/bodyfat.RData')
require(MASS)
colnames(data)<-c('density','bodyFatPercentage','Age','Weight',
'Height','Neck','Chest','Abdomen','Hip','Thigh',
'Knee','Ankle','Biceps','Forearm','Wrist')
ridx <- sample( 1:dim( data )[1], dim( data )[1] )
trainIdx<-ridx[1:200]
testIdx<-ridx[201:length( ridx )]
train <- data[trainIdx,]
test <- data[testIdx,]
KA.test<-read.table('./Data/KnollA-test.dt')
KA.train<-read.table('./Data/KnollA-train.dt')
KB.test<-read.table('./Data/KnollB-test.dt')
KB.train<-read.table('./Data/KnollB-train.dt')
KC.test<-read.table('./Data/KnollC-test.dt')
KC.train<-read.table('./Data/KnollC-train.dt')
require(ggplot2)
qplot(KA.train$V1,KA.train$V2,color=as.factor(KA.train$V3))
qplot(KB.train$V1,KB.train$V2,color=as.factor(KB.train$V3))
qplot(KC.train$V1,KC.train$V2,color=as.factor(KC.train$V3))
qplot(KA.train$V1,KA.train$V2,color=as.factor(KA.train$V3))
qplot(KB.train$V1,KB.train$V2,color=as.factor(KB.train$V3))
qplot(KC.train$V1,KC.train$V2,color=as.factor(KC.train$V3))
qplot(KA.train$V1,KA.train$V2,color=as.factor(KA.train$V3))
qplot(KC.train$V1,KC.train$V2,color=as.factor(KC.train$V3))
qplot(KA.train$V1,KA.train$V2,color=as.factor(KA.train$V3))
qplot(KC.train$V1,KC.train$V2,color=as.factor(KC.train$V3))
require(MASS)
KB.train
ldaa<-lda(as.factor(V3) ~ V1+V2, data = KA.train) #priors=0.5
ldaa<-lda(as.factor(V3) ~ . , data = KA.train) #priors=0.5
?lda
ldaa<-lda(y=as.factor(V3),x=V1+V2, data = KA.train) #priors=0.5
ldaa<-lda(y=as.factor(data$V3),x=data$V1+data$V2) #priors=0.5
KB.train
ldaa<-lda(y=as.factor(V3) ~ . , data = KA.train) #priors=0.5
ldaa<-lda(as.factor(V3) ~ . , data = KA.train) #priors=0.5
lda(as.factor(V3) ~ . , data = KA.train) #priors=0.5
ldaa<-lda(as.factor(V3) ~ . , data = KA.train) #priors=0.5
KA.test
predict(ldaa,KA.test)
predict(ldaa,KA.test)$posterior[,2]
predict(ldaa,KA.test)$posterior[,2]>0.5
which(predict(ldaa,KA.test)$posterior[,2]>0.5)
length(which(predict(ldaa,KA.test)$posterior[,2]>0.5))
predA<-which(predict(ldaa,KA.test)$posterior[,2]>0.5) #indices where class==1
predA
KA.test$V3
KA.test$V3[predA]==1
sum(KA.test$V3[predA]==1)
length(which(KA.test$V3==1))
which(KA.test$V3==1)
KA.test$V3[predA]==1
length(KA.test)
length(KA.test$V3)
ldab<-lda(as.factor(V3) ~ ., data = KB.train)
predB<-which(predict(ldab,KB.test)$posterior[,2]>0.5) #indices where class==1
sum(KB.test$V3[predB]==1)/length(which(KB.test$V3==1)) #accuracy=58% Hey, it's better than half.
ldac<-lda(as.factor(V3) ~ ., data = KC.train)
predC<-which(predict(ldac,KC.test)$posterior[,2]>0.5) #indices where class==1
sum(KC.test$V3[predC]==1)/length(which(KC.test$V3==1)) #accuracy=98%
ldaa
ldaa
ldaa
ldaa$svd
ldaa$xlevels
ldaa$scaling
class(ldaa$scaling)
ldaa$scaling[1]
ldaa$scaling[2]
sum(KC.test$V3[predC]==1)/length(which(KC.test$V3==1)) #accuracy=98%
knollA
KA.train
knnAeuclidian<-apply(X=KA.test,MARGIN=1,FUN=knn,metric='euclidian',train=KA.train,k=3)
knn<-function(test,metric,train,k){
if (metric=='euclidian'){
distance<-function(x,y){
return( sqrt((x[1]-y[1])^2+(x[2]-y[2])^2) )
}
}
if (metric=='alternative'){
distance<-function(x,y){
M=matrix(c(100,0,0,1),2,2)
return(norm(M%*%x-M%*%y)) #f'real?
}
}
trainMat<-as.matrix(cbind(train$V1,train$V2))
d<-vector()
class<-vector()
i=1
while(i<=nrow(trainMat)) {
d<-append(d,(distance(x=trainMat[i,],y=test[c(1,2)])))
class<-append(class,train[i,]$V3)
i=i+1
}
map<-data.frame(cbind(d,class))
map<-map[order(map$d),]
if(sum(map$class[1:k])>0){
return(1)
}
else{
return(-1)
}
}
knnAeuclidian<-apply(X=KA.test,MARGIN=1,FUN=knn,metric='euclidian',train=KA.train,k=3)
length(which(knnAeuclidian==KA.test[,3]))/length(KA.test[,3])
knnAeuclidian<-apply(X=KA.test,MARGIN=1,FUN=knn,metric='euclidian',train=KA.train,k=3)
knnAeuclidian
length(which(knnAeuclidian==KA.test[,3]))/length(KA.test[,3])
knnAalternative<-apply(X=KA.test,MARGIN=1,FUN=knn,metric='alternative',train=KA.train,k=3)
length(which(knnAalternative==KA.test[,3]))/length(KA.test[,3])
knnBeuclidian<-apply(X=KB.test,MARGIN=1,FUN=knn,metric='euclidian',train=KB.train,k=3)
length(which(knnBeuclidian==KB.test[,3]))/length(KB.test[,3])
knnBalternative<-apply(X=KB.test,MARGIN=1,FUN=knn,metric='alternative',train=KB.train,k=3)
length(which(knnBalternative==KB.test[,3]))/length(KB.test[,3])
knnCeuclidian<-apply(X=KC.test,MARGIN=1,FUN=knn,metric='euclidian',train=KC.train,k=3)
length(which(knnCeuclidian==KC.test[,3]))/length(KC.test[,3])
knnCalternative<-apply(X=KC.test,MARGIN=1,FUN=knn,metric='alternative',train=KC.train,k=3)
length(which(knnCalternative==KC.test[,3]))/length(KC.test[,3])
kAaccE <- vector()
kBaccE <- vector()
kCaccE <- vector()
kAaccA <- vector()
kBaccA <- vector()
kCaccA <- vector()
KAY <- 3
while( KAY < 10 )
{
knnAeuclidian<-apply(X=KA.test,MARGIN=1,FUN=knn,metric='euclidian',train=KA.train,k=KAY)
kAaccE<-append( kAaccE, length(which(knnAeuclidian==KA.test[,3]))/length(KA.test[,3]) )
knnAalternative<-apply(X=KA.test,MARGIN=1,FUN=knn,metric='alternative',train=KA.train,k=KAY)
kAaccA<-append( kAaccA, length(which(knnAalternative==KA.test[,3]))/length(KA.test[,3]) )
# k=3
# e:98%
# a:98%
# k=5
# e:97%
# a:98%
# k=7
# e:96%
# a:98%
# k=9
# e:97%
# a:98%
knnBeuclidian<-apply(X=KB.test,MARGIN=1,FUN=knn,metric='euclidian',train=KB.train,k=KAY)
kBaccE<-append( kBaccE, length(which(knnBeuclidian==KB.test[,3]))/length(KB.test[,3]) )
knnBalternative<-apply(X=KB.test,MARGIN=1,FUN=knn,metric='alternative',train=KB.train,k=KAY)
kBaccA<-append( kBaccA, length(which(knnBalternative==KB.test[,3]))/length(KB.test[,3]) )
# k=3
# e:80%
# a:81%
# k=5
# e:81%
# a:84%
# k=7
# e:81%
# a:84%
# k=9
# e:77%
# a:85%
knnCeuclidian<-apply(X=KC.test,MARGIN=1,FUN=knn,metric='euclidian',train=KC.train,k=KAY)
kCaccE<-append( kCaccE, length(which(knnCeuclidian==KC.test[,3]))/length(KC.test[,3]) )
knnCalternative<-apply(X=KC.test,MARGIN=1,FUN=knn,metric='alternative',train=KC.train,k=KAY)
kCaccA<-append( kCaccA, length(which(knnCalternative==KC.test[,3]))/length(KC.test[,3]) )
# k=3
# e:68%
# a:98%
# k=5
# e:66%
# a:98%
# k=7
# e:58%
# a:97%
# k=9
# e:54%
# a:97%
KAY = KAY + 2
}
warnings()
plot(seq(3,9,2),kAaccE,type="line", xlab="k", ylab="Accuracy", main = "Relationship between k and accuracy: Set A, Euclidian metric")
plot(seq(3,9,2),kAaccA,type="line", xlab="k", ylab="Accuracy", main = "Relationship between k and accuracy: Set A, Alternative metric")
plot(seq(3,9,2),kBaccE,type="line", xlab="k", ylab="Accuracy", main = "Relationship between k and accuracy: Set B, Euclidian metric")
plot(seq(3,9,2),kBaccA,type="line", xlab="k", ylab="Accuracy", main = "Relationship between k and accuracy: Set B, Alternative metric")
plot(seq(3,9,2),kCaccE,type="line", xlab="k", ylab="Accuracy", main = "Relationship between k and accuracy: Set C, Euclidian metric")
plot(seq(3,9,2),kCaccA,type="line", xlab="k", ylab="Accuracy", main = "Relationship between k and accuracy: Set C, Alternative metric")
plot(seq(3,9,2),kAaccE,type="line", xlab="k", ylab="Accuracy", main = "Relationship between k and accuracy: KNOLL-A, Euclidian")
plot(seq(3,9,2),kBaccE,type="line", xlab="k", ylab="Accuracy", main = "Relationship between k and accuracy: KNOLL-B, Euclidian")
